{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Bayesian Statistical Analysis\n",
    "\n",
    "Before we jump in to model-building and using MCMC to do wonderful things, it is useful to understand a few of the theoretical underpinnings of the Bayesian statistical paradigm. A little theory (and I do mean a *little*) goes a long way towards being able to apply the methods correctly and effectively.\n",
    "\n",
    "## What *is* Bayesian Statistical Analysis?\n",
    "\n",
    "Though many of you will have taken a statistics course or two during your undergraduate (or graduate) education, most of those who have will likely not have had a course in *Bayesian* statistics. Most introductory courses, particularly for non-statisticians, still do not cover Bayesian methods at all, except perhaps to derive Bayes' formula as a trivial rearrangement of the definition of conditional probability. Even today, Bayesian courses are typically tacked onto the curriculum, rather than being integrated into the program.\n",
    "\n",
    "In fact, Bayesian statistics is not just a particular method, or even a class of methods; it is an entirely different paradigm for doing statistical analysis.\n",
    "\n",
    "> Practical methods for making inferences from data using probability models for quantities we observe and about which we wish to learn.\n",
    "*-- Gelman et al. 2013*\n",
    "\n",
    "A Bayesian model is described by parameters, uncertainty in those parameters is described using probability distributions.\n",
    "\n",
    "All conclusions from Bayesian statistical procedures are stated in terms of *probability statements*\n",
    "\n",
    "![](images/prob_model.png)\n",
    "\n",
    "This confers several benefits to the analyst, including:\n",
    "\n",
    "- ease of interpretation, summarization of uncertainty\n",
    "- can incorporate uncertainty in parent parameters\n",
    "- easy to calculate summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian vs Frequentist Statistics: What's the difference?\n",
    "\n",
    "Any statistical paradigm, Bayesian or otherwise, involves at least the following: \n",
    "\n",
    "1. Some **unknown quantities** about which we are interested in learning or testing. We call these *parameters*.\n",
    "2. Some **data** which have been observed, and hopefully contain information about (1).\n",
    "3. One or more **models** that relate the data to the parameters, and is the instrument that is used to learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frequentist World View\n",
    "\n",
    "![Fisher](images/fisher.png)\n",
    "\n",
    "- The data that have been observed are considered **random**, because they are realizations of random processes, and hence will vary each time one goes to observe the system.\n",
    "- Model parameters are considered **fixed**. The parameters' values are unknown, but they are fixed, and so we *condition* on them.\n",
    "\n",
    "In mathematical notation, this implies a (very) general model of the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "\\\\[f(y | \\theta)\\\\]\n",
    "</div>\n",
    "\n",
    "Here, the model \\\\(f\\\\) accepts data values \\\\(y\\\\) as an argument, conditional on particular values of \\\\(\\theta\\\\).\n",
    "\n",
    "Frequentist inference typically involves deriving **estimators** for the unknown parameters. Estimators are formulae that return estimates for particular estimands, as a function of data. They are selected based on some chosen optimality criterion, such as *unbiasedness*, *variance minimization*, or *efficiency*.\n",
    "\n",
    "> For example, lets say that we have collected some data on the prevalence of autism spectrum disorder (ASD) in some defined population. Our sample includes \\\\(n\\\\) sampled children, \\\\(y\\\\) of them having been diagnosed with autism. A frequentist estimator of the prevalence \\\\(p\\\\) is:\n",
    "\n",
    "> <div style=\"font-size:25px\">\n",
    "> \\\\[\\hat{p} = \\frac{y}{n}\\\\]\n",
    "> </div>\n",
    "\n",
    "> Why this particular function? Because it can be shown to be unbiased and minimum-variance.\n",
    "\n",
    "It is important to note that new estimators need to be derived for every estimand that is introduced.\n",
    "\n",
    "### The Bayesian World View\n",
    "\n",
    "![Bayes](images/bayes.png)\n",
    "\n",
    "- Data are considered **fixed**. They used to be random, but once they were written into your lab notebook/spreadsheet/IPython notebook they do not change.\n",
    "- Model parameters themselves may not be random, but Bayesians use probability distribtutions to describe their uncertainty in parameter values, and are therefore treated as **random**. In some cases, it is useful to consider parameters as having been sampled from probability distributions.\n",
    "\n",
    "This implies the following form:\n",
    "\n",
    "<div style=\"font-size:35px\">\n",
    "\\\\[p(\\theta | y)\\\\]\n",
    "</div>\n",
    "\n",
    "This formulation used to be referred to as ***inverse probability***, because it infers from observations to parameters, or from effects to causes.\n",
    "\n",
    "Bayesians do not seek new estimators for every estimation problem they encounter. There is only one estimator for Bayesian inference: **Bayes' Formula**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Methods in Bayesian Analysis\n",
    "\n",
    "The process of conducting Bayesian inference can be broken down into three general steps (Gelman *et al.* 2013):\n",
    "\n",
    "![](images/123.png)\n",
    "\n",
    "### Step 1: Specify a probability model\n",
    "\n",
    "As was noted above, Bayesian statistics involves using probability models to solve problems. So, the first task is to *completely specify* the model in terms of probability distributions. This includes everything: unknown parameters, data, covariates, missing data, predictions. All must be assigned some probability density.\n",
    "\n",
    "This step involves making choices.\n",
    "\n",
    "- what is the form of the sampling distribution of the data?\n",
    "- what form best describes our uncertainty in the unknown parameters?\n",
    "\n",
    "### Step 2: Calculate a posterior distribution\n",
    "\n",
    "The mathematical form \\\\(p(\\theta | y)\\\\) that we associated with the Bayesian approach is referred to as a **posterior distribution**.\n",
    "\n",
    "> posterior /pos·ter·i·or/ (pos-tēr´e-er) later in time; subsequent.\n",
    "\n",
    "Why posterior? Because it tells us what we know about the unknown \\\\(\\theta\\\\) *after* having observed \\\\(y\\\\).\n",
    "\n",
    "This posterior distribution is formulated as a function of the probability model that was specified in Step 1. Usually, we can write it down but we cannot calculate it analytically. In fact, the difficulty inherent in calculating the posterior distribution for most models of interest is perhaps the major contributing factor for the lack of widespread adoption of Bayesian methods for data analysis. Various strategies for doing so comprise this tutorial.\n",
    "\n",
    "**But**, once the posterior distribution is calculated, you get a lot for free:\n",
    "\n",
    "- point estimates\n",
    "- credible intervals\n",
    "- quantiles\n",
    "- predictions\n",
    "\n",
    "### Step 3: Check your model\n",
    "\n",
    "Though frequently ignored in practice, it is critical that the model and its outputs be assessed before using the outputs for inference. Models are specified based on assumptions that are largely unverifiable, so the least we can do is examine the output in detail, relative to the specified model and the data that were used to fit the model.\n",
    "\n",
    "Specifically, we must ask:\n",
    "\n",
    "- does the model fit data?\n",
    "- are the conclusions reasonable?\n",
    "- are the outputs sensitive to changes in model structure?\n",
    "\n",
    "\n",
    "## Example: binomial calculation\n",
    "\n",
    "Binomial model is suitable for data that are generated from a sequence of exchangeable Bernoulli trials. These data can be summarized by $y$, the number of times the event of interest occurs, and $n$, the total number of trials. The model parameter is the expected proportion of trials that an event occurs.\n",
    "\n",
    "$$p(Y|\\theta) = \\frac{n!}{y! (n-y)!} \\theta^{y} (1-\\theta)^{n-y}$$\n",
    "\n",
    "where $y \\in \\{0, 1, \\ldots, n\\}$ and $p \\in [0, 1]$.\n",
    "\n",
    "To perform Bayesian inference, we require the specification of a prior distribution. A reasonable choice is a uniform prior on [0,1] which has two implications:\n",
    "\n",
    "1. makes all probability values equally probable *a priori* \n",
    "2. makes calculation of the posterior easy\n",
    "\n",
    "The second task in performing Bayesian inference is, given a fully-specified model, to calculate a posterior distribution. As we have specified the model, we can calculate a posterior distribution up to a proportionality constant (that is, a probability distribution that is **unnormalized**):\n",
    "\n",
    "$$P(\\theta | n, y) \\propto P(y | n, \\theta) P(\\theta) = \\theta^y (1-\\theta)^{n-y}$$\n",
    "\n",
    "We can present different posterior distributions as a function of different realized data.\n",
    "\n",
    "We can also calculate posterior estimates for $\\theta$ by maximizing the unnormalized posterior using optimization. \n",
    "\n",
    "### Exercise: posterior estimation\n",
    "\n",
    "Write a function that returns posterior estimates of a binomial sampling model using a uniform prior on the unknown probability. Plot the posterior densities for each of the following datasets:\n",
    "\n",
    "1. n=5, y=3\n",
    "2. n=20, y=12\n",
    "3. n=100, y=60\n",
    "4. n=750, y=450\n",
    "\n",
    "what type of distribution do these plots look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = [5, 20, 100, 750]\n",
    "y = [3, 12, 60, 450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Evaluating Hypotheses with Bayes\n",
    "\n",
    "Statistical inference is a process of learning from incomplete or imperfect (error-contaminated) data. Can account for this \"imperfection\" using either a sampling model or a measurement error model.\n",
    "\n",
    "### Statistical hypothesis testing\n",
    "\n",
    "The *de facto* standard for statistical inference is statistical hypothesis testing. The goal of hypothesis testing is to evaluate a **null hypothesis**. There are two possible outcomes:\n",
    "\n",
    "- reject the null hypothesis\n",
    "- fail to reject the null hypothesis\n",
    "\n",
    "Rejection occurs when a chosen test statistic is higher than some pre-specified threshold valuel; non-rejection occurs otherwise.\n",
    "\n",
    "Notice that neither outcome says anything about the quantity of interest, the **research hypothesis**. \n",
    "\n",
    "Setting up a statistical test involves several subjective choices by the user that are rarely justified based on the problem or decision at hand:\n",
    "\n",
    "- statistical test to use\n",
    "- null hypothesis to test\n",
    "- significance level\n",
    "\n",
    "Choices are often based on arbitrary criteria, including \"statistical tradition\" (Johnson 1999). The resulting evidence is indirect, incomplete, and typically overstates the evidence against the null hypothesis (Goodman 1999).\n",
    "\n",
    "Most importantly to applied users, the results of statistical hypothesis tests are very easy to misinterpret. \n",
    "\n",
    "### Estimation \n",
    "\n",
    "Instead of testing, a more informative and effective approach for inference is based on **estimation** (be it frequentist or Bayesian). That is, rather than testing whether two groups are different, we instead pursue an estimate of *how different* they are, which is fundamentally more informative. \n",
    "\n",
    "Additionally, we include an estimate of **uncertainty** associated with that difference which includes uncertainty due to our lack of knowledge of the model parameters (*epistemic uncertainty*) and uncertainty due to the inherent stochasticity of the system (*aleatory uncertainty*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## One Group\n",
    "\n",
    "Before we compare two groups using Bayesian analysis, let's start with an even simpler scenario: statistical inference for one group.\n",
    "\n",
    "For this we will use Gelman et al.'s (2007) radon dataset. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\n",
    "\n",
    ">  the US EPA has set an action level of 4 pCi/L. At or above this level of radon, the EPA recommends you take corrective measures to reduce your exposure to radon gas.\n",
    "\n",
    "![radon](images/how_radon_enters.jpg)\n",
    "\n",
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEEDS = 20090425, 19700903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "radon = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "radon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's focus on the (log) radon levels measured in a single county (Hennepin). \n",
    "\n",
    "Suppose we are interested in:\n",
    "\n",
    "- whether the mean log-radon value is greater than 4 pCi/L in Hennepin county\n",
    "- the probability that any randomly-chosen household in Hennepin county has a reading of greater than 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hennepin_radon = radon.query('county==\"HENNEPIN\"').log_radon\n",
    "sns.distplot(hennepin_radon);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The model\n",
    "\n",
    "Recall that the first step in Bayesian inference is specifying a **full probability model** for the problem.\n",
    "\n",
    "This consists of:\n",
    "\n",
    "- a likelihood function(s) for the observations\n",
    "- priors for all unknown quantities\n",
    "\n",
    "The measurements look approximately normal, so let's start by assuming a normal distribution as the sampling distribution (likelihood) for the data. \n",
    "\n",
    "$$y_i \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "(don't worry, we can evaluate this assumption)\n",
    "\n",
    "This implies that we have 2 unknowns in the model; the mean and standard deviation of the distribution. \n",
    "\n",
    "#### Prior choice\n",
    "\n",
    "How do we choose distributions to use as priors for these parameters? \n",
    "\n",
    "There are several considerations:\n",
    "\n",
    "- discrete vs continuous values\n",
    "- the support of the variable\n",
    "- the available prior information\n",
    "\n",
    "While there may likely be prior information about the distribution of radon values, we will assume no prior knowledge, and specify a **diffuse** prior for each parameter.\n",
    "\n",
    "Since the mean can take any real value (since it is on the log scale), we will use another normal distribution here, and specify a large variance to allow the possibility of very large or very small values:\n",
    "\n",
    "$$\\mu \\sim N(0, 10^2)$$\n",
    "\n",
    "For the standard deviation, we know that the true value must be positive (no negative variances!). I will choose a uniform prior bounded from below at zero and from above at a value that is sure to be higher than any plausible value the true standard deviation (on the log scale) could take.\n",
    "\n",
    "$$\\sigma \\sim U(0, 10)$$\n",
    "\n",
    "We can encode these in a Python model, using the PyMC3 package, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, Uniform\n",
    "\n",
    "with Model() as radon_model:\n",
    "    \n",
    "    μ = Normal('μ', mu=0, sd=5)\n",
    "    σ = Uniform('σ', 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> ## Software\n",
    "> Today there is an array of software choices for Bayesians, including both open source software (*e.g.*, Stan, PyMC, JAGS, emcee) and commercial (*e.g.*, SAS, Stata). These examples can be replicated in any of these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All that remains is to add the likelihood, which takes $\\mu$ and $\\sigma$ as parameters, and the log-radon values as the set of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with radon_model:\n",
    "    \n",
    "    dist = Normal('dist', mu=μ, sd=σ, observed=hennepin_radon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead and estimate the model paramters from the data, its a good idea to perform a **prior predictive check**. This involves sampling from the model before data are incorporated, and gives you an idea of the range of observations that would be considered reasonable within the scope of the modeling assumptions (including choice of priors). If the simnulations generate too many extreme observations relative to our expectations based on domain knowledge, then it can be an indication of problems with model formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import sample_prior_predictive\n",
    "\n",
    "with radon_model:\n",
    "    \n",
    "    prior_sample = sample_prior_predictive(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(prior_sample['dist'].ravel(), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(radon.log_radon, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, we will fit the model using **Markov chain Monte Carlo (MCMC)**, which will be covered in detail in an upcoming section. This will draw samples from the posterior distribution (which cannot be calculated exactly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import sample\n",
    "\n",
    "with radon_model:\n",
    "    \n",
    "    samples = sample(1000, tune=1000, cores=2, random_seed=RANDOM_SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from arviz import plot_posterior\n",
    "\n",
    "plot_posterior(samples, var_names=['μ'], ref_val=np.log(4), credible_interval=0.95, kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The plot shows the posterior distribution of $\\mu$, along with an estimate of the 95% posterior **credible interval**. \n",
    "\n",
    "The output\n",
    "\n",
    "    85% < 1.38629 < 15%\n",
    "    \n",
    "informs us that the probability of $\\mu$ being less than log(4) is 85% and the corresponding probability of being greater than log(4) is 15%.\n",
    "\n",
    "> The posterior probability that the mean level of household radon in Henneprin County is greater than 4 pCi/L is 0.15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prediction\n",
    "\n",
    "What is the probability that a given household has a log-radon measurement larger than one? To answer this, we make use of the **posterior predictive distribution**.\n",
    "\n",
    "$$p(z |y) = \\int_{\\theta} p(z |\\theta) p(\\theta | y) d\\theta$$\n",
    "\n",
    "where here $z$ is the predicted value and y is the data used to fit the model.\n",
    "\n",
    "We can estimate this from the posterior samples of the parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mus = samples['μ']\n",
    "sigmas = samples['σ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "radon_samples = Normal.dist(mus, sigmas).random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(radon_samples > np.log(4)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> The posterior probability that a randomly-selected household in Henneprin County contains radon levels in excess of 4 pCi/L is 0.48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model checking\n",
    "\n",
    "But, ***how do we know this model is any good?***\n",
    "\n",
    "Its important to check the fit of the model, to see if its assumptions are reasonable. One way to do this is to perform **posterior predictive checks**. This involves generating simulated data using the model that you built, and comparing that data to the observed data.\n",
    "\n",
    "One can choose a particular statistic to compare, such as tail probabilities or quartiles, but here it is useful to compare them graphically.\n",
    "\n",
    "We already have these simulations from the previous exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(radon_samples, label='simulated')\n",
    "sns.distplot(hennepin_radon, label='observed')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prior sensitivity\n",
    "\n",
    "Its also important to check the sensitivity of your choice of priors to the resulting inference.\n",
    "\n",
    "Here is the same model, but with drastically different (though still uninformative) priors specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Flat, HalfCauchy\n",
    "\n",
    "with Model() as prior_sensitivity:\n",
    "    \n",
    "    μ = Flat('μ')\n",
    "    σ = HalfCauchy('σ', 5)\n",
    "    \n",
    "    dist = Normal('dist', mu=μ, sd=σ, observed=hennepin_radon)\n",
    "    \n",
    "    sensitivity_samples = sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(sensitivity_samples, var_names=['μ'], ref_val=np.log(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is the original model for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(samples, var_names=['μ'], ref_val=np.log(4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Two Groups with Continiuous Outcome\n",
    "\n",
    "To illustrate how this Bayesian estimation approach works in practice, we will use a fictitious example from Kruschke (2012) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "drug = pd.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pd.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "\n",
    "trial_data = pd.concat([drug, placebo], ignore_index=True)\n",
    "trial_data.hist('iq', by='group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since there appear to be extreme (\"outlier\") values in the data, we will choose a Student-t distribution to describe the distributions of the scores in each group. This sampling distribution adds **robustness** to the analysis, as a T distribution is less sensitive to outlier observations, relative to a normal distribution. \n",
    "\n",
    "The three-parameter Student-t distribution allows for the specification of a mean $\\mu$, a precision (inverse-variance) $\\lambda$ and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "the degrees-of-freedom parameter essentially specifies the \"normality\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails.\n",
    "\n",
    "Thus, the likelihood functions of our model are specified as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(drug)}_i &\\sim T(\\nu, \\mu_1, \\sigma_1) \\\\\n",
    "y^{(placebo)}_i &\\sim T(\\nu, \\mu_2, \\sigma_2)\n",
    "\\end{align}$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups. \n",
    "\n",
    "### Prior choice\n",
    "\n",
    "Since the means are real-valued, we will apply normal priors. Since we know something about the population distribution of IQ values, we will center the priors at 100, and use a standard deviation that is more than wide enough to account for plausible deviations from this population mean:\n",
    "\n",
    "$$\\mu_k \\sim N(100, 10^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with Model() as drug_model:\n",
    "    \n",
    "    μ_0 = Normal('μ_0', 100, sd=10)\n",
    "    μ_1 = Normal('μ_1', 100, sd=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Similarly, we will use a uniform prior for the standard deviations, with an upper bound of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    σ_0 = Uniform('σ_0', lower=0, upper=20)\n",
    "    σ_1 = Uniform('σ_1', lower=0, upper=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For the degrees-of-freedom parameter $\\nu$, we will use an **exponential** distribution with a mean of 30; this allocates high prior probability over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Exponential\n",
    "\n",
    "with drug_model:\n",
    "    ν = Exponential('ν_minus_one', 1/29.) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(Exponential.dist(1/29).random(size=10000), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import StudentT\n",
    "\n",
    "with drug_model:\n",
    "\n",
    "    drug_like = StudentT('drug_like', nu=ν, mu=μ_1, lam=σ_1**-2, observed=drug.iq)\n",
    "    placebo_like = StudentT('placebo_like', nu=ν, mu=μ_0, lam=σ_0**-2, observed=placebo.iq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that the model is fully specified, we can turn our attention to tracking the posterior quantities of interest. Namely, we can calculate the difference in means between the drug and placebo groups.\n",
    "\n",
    "As a joint measure of the groups, we will also estimate the \"effect size\", which is the difference in means scaled by the pooled estimates of standard deviation. This quantity can be harder to interpret, since it is no longer in the same units as our data, but it is a function of all four estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with drug_model:\n",
    "    \n",
    "    diff_of_means = Deterministic('difference of means', μ_1 - μ_0)\n",
    "    \n",
    "    effect_size = Deterministic('effect size', \n",
    "                        diff_of_means / np.sqrt((σ_1**2 + σ_0**2) / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    drug_trace = sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], var_names=['μ_0', 'μ_1', 'σ_0', 'σ_1', 'ν_minus_one']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], \n",
    "          var_names=['difference of means', 'effect size'],\n",
    "          ref_val=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> The posterior probability that the mean IQ of subjects in the treatment group is greater than that of the control group is 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Exercise: Two Groups with Binary Outcome\n",
    "\n",
    "Now that we have seen how to generalize normally-distributed data to another distribution, we are equipped to analyze other data types. Binary outcomes are common in clinical research: \n",
    "\n",
    "- survival/death\n",
    "- true/false\n",
    "- presence/absence\n",
    "- positive/negative\n",
    "\n",
    "In practice, binary outcomes are encoded as ones (for event occurrences) and zeros (for non-occurrence). A single binary variable is distributed as a **Bernoulli** random variable:\n",
    "\n",
    "$$f(x \\mid p) = p^{x} (1-p)^{1-x}$$\n",
    "\n",
    "Such events are sometimes reported as sums of individual events, such as the number of individuals in a group who test positive for a condition of interest. Sums of Bernoulli events are distributed as **binomial** random variables.\n",
    "\n",
    "$$f(x \\mid n, p) = \\binom{n}{x} p^x (1-p)^{n-x}$$\n",
    "\n",
    "The parameter in both models is $p$, the probability of the occurrence of an event. In terms of inference, we are typically interested in whether $p$ is larger or smaller in one group relative to another.\n",
    "\n",
    "To demonstrate the comparison of two groups with binary outcomes using Bayesian inference, we will use a sample pediatric dataset. Data on 671 infants with very low (<1600 grams) birth weight from 1981-87 were collected at Duke University Medical Center. Of interest is the relationship between the outcome intra-ventricular hemorrhage (IVH) and predictor such as birth weight, gestational age, presence of pneumothorax and mode of delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv('../data/vlbw.csv', index_col=0).dropna(axis=0, subset=['ivh', 'pneumo'])\n",
    "vlbw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To demonstrate binary data analysis, we will try to estimate the difference between the probability of an intra-ventricular hemorrhage for infants with a pneumothorax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(vlbw.ivh, vlbw.pneumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will create a binary outcome by combining `definite` and `possible` into a single outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ivh = vlbw.ivh.isin(['definite', 'possible']).astype(int).values\n",
    "x = vlbw.pneumo.astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a model that evaluates the association of a pneumothorax with the probability of IVH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prior choice\n",
    "\n",
    "What should we choose as a prior distribution for $p$?\n",
    "\n",
    "We could stick with a normal distribution, but note that the value of $p$ is **constrained** by the laws of probability. Namely, we cannot have values smaller than zero nor larger than one. So, choosing a normal distribution will result in ascribing positive probability to unsupported values of the parameter. In many cases, this will still work in practice, but will be inefficient for calculating the posterior and will not accurately represent the prior information about the parameter.\n",
    "\n",
    "A common choice in this context is the **beta distribution**, a continuous distribution with 2 parameters and whose support is on the unit interval:\n",
    "\n",
    "$$ f(x \\mid \\alpha, \\beta) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "- Support: $x \\in (0, 1)$\n",
    "- Mean: $\\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "- Variance: $\\dfrac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Beta\n",
    "\n",
    "params = (5, 1), (1, 3), (5, 5), (0.5, 0.5), (1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(params), figsize=(14, 4), sharey=True)\n",
    "for ax, (alpha, beta) in zip(axes, params):\n",
    "    sns.distplot(Beta.dist(alpha, beta).random(size=10000), ax=ax, kde=False)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(r'$\\alpha={0}, \\beta={1}$'.format(alpha, beta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So let's use a beta distribution to model our prior knowledge of the probabilities for both groups. Setting $\\alpha = \\beta = 1$ will result in a uniform distribution of prior mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with Model() as ivh_model:\n",
    "    \n",
    "    p = Beta('p', 1, 1, shape=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can now use `p` as the parameter of our Bernoulli likelihood. Here, `x` is a vector of zeros an ones, which will extract the approproate group probability for each subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pymc3 import Bernoulli\n",
    "\n",
    "with ivh_model:\n",
    "    \n",
    "    bb_like = Bernoulli('bb_like', p=p[x], observed=ivh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, since we are interested in the difference between the probabilities, we will keep track of this difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    \n",
    "    p_diff = Deterministic('p_diff', p[1] - p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    ivh_trace = sample(1000, init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], var_names=['p']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that the probability that `p` is larger for the pneumothorax with probability one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], var_names=['p_diff'], ref_val=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As a final demonstration, let's make this analysis slightly more realistic. Since these are observational data, its probably a bad idea to compare group probabilities without correcting them for **confounding variables**. Individuals may have systematic differences between groups that may explain the difference in observed outcomes that have nothing to do with group membership itself. \n",
    "\n",
    "For example, perhaps lower birthweight accounts for the difference, since birthweight is nominally lower in the pneumothorax group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vlbw.groupby('pneumo').bwt.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's add birth weight as a predictor to our model (but first we will standardize it to aid interpretation and for numerical stability):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bwt_norm = (vlbw.bwt - vlbw.bwt.mean()) / vlbw.bwt.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How do we add a continuous covariate to this model?\n",
    "\n",
    "The easiest way to do so is to change the formulation of the model to that of a **generalized linear model (GLM)**. What this does is create a regression model to predict the latent probability of IVH for each individual. \n",
    "\n",
    "$$p_i = \\mu + \\alpha * \\text{bwt}_i + \\beta * \\text{pneumo}_i$$\n",
    "\n",
    "here, $\\mu$ is a baseline probability, $\\alpha$ is a coefficient for centered birthweight, and $\\beta$ is a coefficient for pneumothorax.\n",
    "\n",
    "However, this presents a problem as formulated: its very easy to generate combinations of $\\mu$, $\\alpha$, and $\\beta$ that generate values outside the $(0,1)$ interval. To avoid this, we must **transform** this sum in such a way that the unit interval constraint is guaranteed.\n",
    "\n",
    "One such transformation is the **logit**:\n",
    "\n",
    "$$x = \\text{logit}(p) = \\log\\left[\\frac{p}{1-p}\\right]$$\n",
    "\n",
    "the inverse of this function maps real values of $x$ to values of $p$ on the unit interval:\n",
    "\n",
    "$$p = \\text{invlogit}(x) = \\frac{1}{1 + \\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(-7,7)\n",
    "plt.plot(x_range, 1/(1+np.exp(-x_range)))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('p');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To convert our original model to a GLM, we need to specify priors for the linear model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with Model() as ivh_glm:\n",
    "    \n",
    "    μ = Normal('μ', 0, sd=5)\n",
    "    α = Normal('α', 0, sd=5)\n",
    "    β = Normal('β', 0, sd=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Then, apply the `invlogit` transformation to the linear combination of predictors.\n",
    "\n",
    "The likelihood stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymc3.math import invlogit\n",
    "\n",
    "with ivh_glm:\n",
    "        \n",
    "    p = invlogit(μ + α*bwt_norm + β*x)\n",
    "    \n",
    "    bb_like = Bernoulli('bb_like', p=p, observed=ivh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with ivh_glm:\n",
    "    trace_glm = sample(1000, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can see that the estimate for $\\alpha$ is negative, indicating that higher birth weight reduces the baseline probability of IVH. \n",
    "\n",
    "The coefficient for $\\beta$ is strongly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_posterior(trace_glm[100:], var_names=['μ', 'α', 'β']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How might we represent the effect of pneumothorax as a quantity on the probability scale, to aid interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# References\n",
    "\n",
    "Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. CRC Press.\n",
    "\n",
    "Pilon, Cam-Davidson. [Probabilistic Programming and Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
